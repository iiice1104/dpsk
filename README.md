# dpsk-R1

# 1.支持128K输入上下文长度

RoPE(Rotary Position Embedding)：旋转位置编码

外推性：模型对超出训练范围的数据的泛化性能，对于RoPE指的是处理比训练时更长的序列时的有效性

核心问题：比训练时更长的序列意味着模型需要处理未见过的位置索引
位置编码的重要性：模型通过位置编码来理解token的顺序关系，若外推性不足，模型难以捕捉长距离位置依赖或相对位置关系

如果外推性较差：则模型处理长文本生成任务时可能出现语段重复、逻辑混乱的情况

RoPE通过几何旋转将位置信息融入向量，旋转角度具有周期性，即使位置索引m超出范围，也能通过旋转将其处理到合理范围内，内积结果依赖相对位置差而不是绝对位置，从而外推性表现好(暂时不理解为什么RoPE的外推性好)

YaRN(Yet another RoPE extensioN):结合NTK(动态调整旋转角度基数)和位置插值（将位置索引m压缩成m/λ到训练范围），进一步扩展上下文窗口（模型单次处理时可以考虑的最大token数量，即捕捉信息的范围）
RoPE的外推性具有局限性，YaRN在RoPE的基础上采用多阶段插值策略和动态调整机制，具有更优的外推性，在超出长度的序列任务上表现优异

优化措施中的一些概念：  
高频维度：RoPE中旋转角度较大，旋转速度较快的维度，一般是隐藏层的低维部分，索引较小，擅长捕捉局部细节和近距离依赖关系  
低频维度：相应地，旋转角度较小，旋转较慢，维度索引较大，捕捉长距离依赖和全局结构  
波长：设旋转角度为θi，则旋转一周是2Π，波长λ是在维度i上旋转一周（2Π）需要的序列长度。例如当λ=100，则每100个token后改维度的位置编码会循环一次  
短波长（高频维度）：旋转角度大，波长短，适合编码局部位置  
长波长（低频维度）：旋转角度小，波长长，适合编码全局位置  

优化策略：根据波长λ与预训练长度的关系，将序列分为三个部分，较短波长即索引较小部分保留原始编码，中等波长部分插值，较长波长即索引较大部分完全插值

# 2.多头潜在注意力机制Mutile-Head-Latent-Attention MLA

传统的Transformer采用多头注意力机制MHA，但键值（KV）缓存限制推理速度，由此dpsk引入了MLA，性能更优且KV缓存大大减少

首先介绍多头注意力机制MHA:将输入序列通过多组独立的注意力计算处理，每组称为一个“头”，每个头关注不同的内容不同的依赖关系，所有头的结果合并输出为最终注意力输出

1.首先输入为X，经过三种不同的线性变换，即使用权重矩阵将输入向量X投影到不同子空间分别得到查询Q，键K，值V，按照头的数量h，在特征维度上分为h份，每一份的维度为d_k=d_models/h  
2.单个头注意力计算：分数矩阵=Q·K^T，防止点积结果太大导致softmax梯度消失，进行缩放，得到缩放分数=Q·K^T/√d_k，注意力权重=softmax(缩放分数），输出=注意力权重·V  
3.将所有头的输出拼接起来，拼接输出=[head1,head2,...headh],然后使用线性变换Wo将其融合，最终输出=拼接输出·Wo,映射到原始维度  

接下来介绍MLA:MHA需要为每个注意力头缓存完整的KV矩阵，显存占用随着头数线性增长，MLA通过低秩联合压缩技术，将KV矩阵压缩为潜在向量，显著降低空间占用  
1.下投影：对于输入h，使用共享的降维矩阵W<sup>dkv</sup>，将原始高维特征映射到低维潜在空间，得C<sup>KV</sup>=W<sup>DKV</sup>h,同理C<sup>Q</sup>=W<sup>DKV</sup>h，缓存的是降维结果  
2.向上投影：后续推理使用时，需要将潜在向量升维，使用头特定的升维矩阵W<sup>kk</sup>、W<sup>vv</sup>等，例如计算分数矩阵时，Q·K^T=Q·(W<sup>kk</sup>C<sup>KV</sup>)^T  
3.与RoPE兼容：  
RoPE不是应用于上投影得到的q<sup>C</sup>，而是直接从C<sup>Q</sup>生成新的Q嵌入q<sup>R</sup>:q<sup>R</sup>=RoPE(W<sup>QR</sup>C<sup>Q</sup>)  
对于新的k嵌入，也不是应用于上投影后的K，与q不同，也不是下投影的K(C<sup>K</sup>)，而是由输入h生成：k<sup>R</sup>=RoPE(W<sup>KR</sup>h) 


